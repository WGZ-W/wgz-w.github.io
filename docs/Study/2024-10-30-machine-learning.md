---
layout: post
tag: study
title: "Machine Learning"
---


# Machine Learning

- [Machine Learning](#machine-learning)
  - [Mind Map](#mind-map)
  - [支持向量机 (Support Vector Machine)](#支持向量机-support-vector-machine)
  - [反向传播算法 (backward propagation, BP)](#反向传播算法-backward-propagation-bp)
    - [ChatGPT Principle](#chatgpt-principle)
    - [Professional Words](#professional-words)
  - [回归 Regression](#回归-regression)
    - [线性模型 Linear model](#线性模型-linear-model)
    - [梯度下降 Gradient Descent](#梯度下降-gradient-descent)
    - [正则化 Regularization](#正则化-regularization)
  - [分类 Classification](#分类-classification)

## Mind Map
[Machine Learning Mind Map](/mindmap/machine-learning.html)

## 支持向量机 (Support Vector Machine)
## 反向传播算法 (backward propagation, BP)
Backpropagation, Gradient Descent,
- Chain Rule
- Forward pass
- Backward pass

### ChatGPT Principle
ChatGPT 的原理，就是文字接龙（语言模型），通过输入，输出一个机率分布（Probability distribution），然后随机 取样（sample）一个字，加到输入后面，然后再输入到模型中。

GPT 使用预训练（Pre-train）, 或者叫自监督学习（Self-supervised learning）, 生成的模型叫做基石模型（Foundation model）。
预训练作用：多语言习得，在一种语言上训练，其他语言自动学习。

微调（Finetune），在基石模型（Foundation model）上继续学习。

监督学习（Supervised Learning），就是提供成对的学习资料。  

强化学习（Reinforcement Learning (RL)）：
我们在某些任务上不知道如何标注数据，但是我们可以提供判断结果的好坏。（It is challenging to label data in some tasks. We can know the results are good or not.）


### Professional Words
- Machine Learning ~= Looking for Function  
- Deep Learning
- Neural Network
- Downstream tasks
- Generative Adversarial Network
- Anomaly Detection
- Explainable AI
- Model Attack
- Domain Adaptation
- Network Compression
- Life-long Learning
- Meta Learning = Learn to Learn
- Few-shot learning

## 回归 Regression
### 线性模型 Linear model
### 梯度下降 Gradient Descent
### 正则化 Regularization

## 分类 Classification







